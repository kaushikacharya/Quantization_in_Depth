{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer\n",
    "\n",
    "In this lesson, you will learn how to compress any model in 8-bit precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: class `W8A16LinearLayer`\n",
    "\n",
    "- Build the target class, `W8A16LinearLayer()`, that will be responsible for quantizing your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - `w8_a16_forward` Function\n",
    "\n",
    "-\n",
    "```Python\n",
    "W8A16LinearLayer\n",
    "                    # 8-bit  # 16-bit         # optional\n",
    "* w8_a16_forward -> weights, input,   scales, bias=None\n",
    "                    \n",
    "```\n",
    "- Cast the 8-bit `weights` to the same data type as the `input`, \"casted weights\",\n",
    "- keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
    "- Next, $$(({inputs} \\cdot \\text{``casted weights''}) * {scale}) + {bias}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(low=-128, high=127, size=(32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn(size=(1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn(size=(1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn(size=(1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 219.0000, -282.0000, -148.0000,  -28.2500,   17.0000,  129.0000,\n",
       "          -96.0000,  282.0000, -216.0000, -568.0000,  -53.0000, -270.0000,\n",
       "         -334.0000,  -48.0000, -390.0000,  488.0000,   77.0000, -170.0000,\n",
       "          -36.2500, -450.0000, -294.0000,  338.0000, -270.0000, -470.0000,\n",
       "         -256.0000,   75.5000,  354.0000, -568.0000, -157.0000,   61.5000,\n",
       "         -320.0000,  -49.5000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(input=random_hs, weight=random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-157.0000, -131.0000,  -69.0000,   35.7500,  -18.0000,  134.0000,\n",
       "           12.1875,  -94.0000,  204.0000, -440.0000,   31.6250,  -76.5000,\n",
       "          -28.2500,   72.5000,  564.0000, -356.0000, -100.0000,   10.6875,\n",
       "            2.8125, -272.0000, -255.0000,   82.0000, -398.0000,  352.0000,\n",
       "           57.5000,   -4.6875,  -79.0000, -330.0000,  127.5000,  118.0000,\n",
       "           91.0000,   -7.6875]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(input=random_hs, weight=random_int8.to(random_hs.dtype)) * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-158.0000, -131.0000,  -69.0000,   35.7500,  -16.6250,  134.0000,\n",
       "           12.1250,  -94.0000,  204.0000, -442.0000,   31.8750,  -78.5000,\n",
       "          -26.0000,   72.0000,  564.0000, -356.0000, -101.0000,   10.7500,\n",
       "            4.3750, -272.0000, -256.0000,   81.5000, -398.0000,  352.0000,\n",
       "           58.2500,   -6.7500,  -79.5000, -330.0000,  128.0000,  118.5000,\n",
       "           90.5000,   -7.8750]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(input=random_hs, weight=random_int8.to(random_hs.dtype)) * scales + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement all this as a function, `w8_a16_forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input=input, weight=casted_weights) * scales\n",
    "\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bias: tensor([[-158.0000, -131.0000,  -69.0000,   35.7500,  -16.6250,  134.0000,\n",
      "           12.1250,  -94.0000,  204.0000, -442.0000,   31.8750,  -78.5000,\n",
      "          -26.0000,   72.0000,  564.0000, -356.0000, -101.0000,   10.7500,\n",
      "            4.3750, -272.0000, -256.0000,   81.5000, -398.0000,  352.0000,\n",
      "           58.2500,   -6.7500,  -79.5000, -330.0000,  128.0000,  118.5000,\n",
      "           90.5000,   -7.8750]], dtype=torch.bfloat16)\n",
      "Without bias: tensor([[-157.0000, -131.0000,  -69.0000,   35.7500,  -18.0000,  134.0000,\n",
      "           12.1875,  -94.0000,  204.0000, -440.0000,   31.6250,  -76.5000,\n",
      "          -28.2500,   72.5000,  564.0000, -356.0000, -100.0000,   10.6875,\n",
      "            2.8125, -272.0000, -255.0000,   82.0000, -398.0000,  352.0000,\n",
      "           57.5000,   -4.6875,  -79.0000, -330.0000,  127.5000,  118.0000,\n",
      "           91.0000,   -7.6875]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"With bias: {w8_a16_forward(weight=random_int8, input=random_hs, scales=scales, bias=bias)}\")\n",
    "\n",
    "print(f\"Without bias: {w8_a16_forward(weight=random_int8, input=random_hs, scales=scales)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - `init` Function of class `W8A16LinearLayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the `init` is of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
    "```Python\n",
    "def __init__(self, in_features, out_features, bias=True,\n",
    "             device=None, dtype=None)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# running this will result in an error\n",
    "class W8A16LinearLayer:\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.int8_weights = nn.Parameter(data=torch.Tensor([0, 1]).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    W8A16LinearLayer(in_features=1, out_features=1)\n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the weights as buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weights\",\n",
    "                              torch.randint(low=-128, high=127, size=(out_features, in_features), dtype=torch.int8)\n",
    "                              )\n",
    "        \n",
    "        self.register_buffer(\"scales\",\n",
    "                             torch.randn((1, out_features), dtype=dtype)\n",
    "                             )\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\",\n",
    "                                 torch.randn((1, out_features), dtype=dtype)\n",
    "                                 )\n",
    "        else:\n",
    "            self.bias = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instance = W8A16LinearLayer(in_features=16, out_features=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(dummy_instance.int8_weights.shape)\n",
    "print(dummy_instance.scales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - `forward` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- Use the `w8_a16_forward` defined earlier (Step 1.1) to define the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weights\",\n",
    "                              torch.randint(low=-128, high=127, size=(out_features, in_features), dtype=torch.int8)\n",
    "                              )\n",
    "        \n",
    "        self.register_buffer(\"scales\",\n",
    "                             torch.randn((1, out_features), dtype=dtype)\n",
    "                             )\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\",\n",
    "                                 torch.randn((1, out_features), dtype=dtype)\n",
    "                                 )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(weight=self.int8_weights, input=input, scales=self.scales, bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(in_features=16, out_features=32)\n",
    "dummy_hidden_states = torch.randn(1,6,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KA: Check that output has same data type as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(in_features=16, out_features=32, dtype=torch.bfloat16)\n",
    "dummy_hidden_states = torch.randn(1,6,16, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - `quantize` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- `quantize` function will dynamically quantize half-precision weights into `torch.int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weights\",\n",
    "                              torch.randint(low=-128, high=127, size=(out_features, in_features), dtype=torch.int8)\n",
    "                              )\n",
    "        \n",
    "        self.register_buffer(\"scales\",\n",
    "                             torch.randn((1, out_features), dtype=dtype)\n",
    "                             )\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\",\n",
    "                                 torch.randn((1, out_features), dtype=dtype)\n",
    "                                 )\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights/scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(weight=self.int8_weights, input=input, scales=self.scales, bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(in_features=4, out_features=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before:\n",
      " tensor([[ -13,  -71,  -29,  108],\n",
      "        [ -23,  -54,   22,  -64],\n",
      "        [-116,  -81,  -32,   85],\n",
      "        [ -79,  -48,  -75,  126],\n",
      "        [  26,  -52,  -16,  -70],\n",
      "        [  44,  112,  -25,   -8],\n",
      "        [ -76,  -34,  -51,   81],\n",
      "        [ -37,   73,    6,   22]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights before:\\n\", module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8750,  0.1396, -0.3438,  0.4395, -0.9570, -0.6875,  0.5117, -0.3145],\n",
       "        [-0.1953,  0.7031,  0.8945, -1.6797, -1.0078,  2.0781,  0.6562,  1.8125],\n",
       "        [ 0.4648,  0.1904, -1.5781, -0.9609,  1.3281,  0.6211,  0.4414, -0.5508],\n",
       "        [-1.7734,  0.6953,  0.4824, -0.8672,  0.3320, -0.1797, -0.0286, -0.9570]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.quantize(weights=random_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after:\n",
      " tensor([[ 116,   18,  -46,   58, -127,  -91,   68,  -42],\n",
      "        [ -12,   43,   55, -102,  -62,  127,   40,  111],\n",
      "        [  37,   15, -126,  -77,  106,   50,   36,  -44],\n",
      "        [-127,   50,   34,  -62,   24,  -13,   -2,  -68]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weights after:\\n\", module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0075, 0.0164, 0.0125, 0.0140], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8750,  0.1357, -0.3477,  0.4375, -0.9570, -0.6875,  0.5117, -0.3164],\n",
       "        [-0.1963,  0.7031,  0.8984, -1.6719, -1.0156,  2.0781,  0.6562,  1.8125],\n",
       "        [ 0.4609,  0.1865, -1.5703, -0.9570,  1.3203,  0.6211,  0.4492, -0.5469],\n",
       "        [-1.7734,  0.6992,  0.4746, -0.8672,  0.3359, -0.1816, -0.0280, -0.9492]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### dequantized weights\n",
    "module.int8_weights * module.scales.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8750,  0.1396, -0.3438,  0.4395, -0.9570, -0.6875,  0.5117, -0.3145],\n",
       "        [-0.1953,  0.7031,  0.8945, -1.6797, -1.0078,  2.0781,  0.6562,  1.8125],\n",
       "        [ 0.4648,  0.1904, -1.5781, -0.9609,  1.3281,  0.6211,  0.4414, -0.5508],\n",
       "        [-1.7734,  0.6953,  0.4824, -0.8672,  0.3320, -0.1797, -0.0286, -0.9570]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### original weights\n",
    "random_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average quantization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0030, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_matrix - module.int8_weights * module.scales.unsqueeze(1)).abs().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_quantization_in_depth_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
